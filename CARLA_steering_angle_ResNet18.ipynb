{"cells":[{"cell_type":"markdown","id":"c0a7f7c0","metadata":{"id":"c0a7f7c0"},"source":["## import library"]},{"cell_type":"code","execution_count":null,"id":"a8f47f24","metadata":{"id":"a8f47f24"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","\n","from PIL import Image\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","id":"4816626b","metadata":{"id":"4816626b"},"source":["## Download dataset"]},{"cell_type":"code","execution_count":null,"id":"ecce9ff3","metadata":{"id":"ecce9ff3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756199342789,"user_tz":-420,"elapsed":113698,"user":{"displayName":"Phương Lê Huy","userId":"18219861464535664958"}},"outputId":"c53f104a-ca60-4fdb-d182-ffc7bb755770"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1aAyaNM8q54G5S2IKtw2XAv4BmuEbidX0\n","From (redirected): https://drive.google.com/uc?id=1aAyaNM8q54G5S2IKtw2XAv4BmuEbidX0&confirm=t&uuid=c7c7863b-3601-4498-9a27-b1d60f5b88ef\n","To: /content/Data.zip\n","100% 7.74G/7.74G [01:41<00:00, 76.1MB/s]\n"]}],"source":["!pip install -q gdown\n","!gdown --id 1aAyaNM8q54G5S2IKtw2XAv4BmuEbidX0"]},{"cell_type":"markdown","id":"2414d1e4","metadata":{"id":"2414d1e4"},"source":["## Unzip"]},{"cell_type":"code","execution_count":null,"id":"fce53f64","metadata":{"id":"fce53f64"},"outputs":[],"source":["import zipfile\n","\n","with zipfile.ZipFile(\"Data.zip\", 'r') as zip_ref:\n","    zip_ref.extractall(\"/content/Data\")  # This will extract into ./dataset"]},{"cell_type":"markdown","id":"e725505c","metadata":{"id":"e725505c"},"source":["## Preprocess data"]},{"cell_type":"code","execution_count":null,"id":"02b335b4","metadata":{"id":"02b335b4"},"outputs":[],"source":["steer_file_path = 'Data/Data/SteerValues/steer_values.txt'\n","with open(steer_file_path, 'r') as f:\n","    steer_values = [float(line.strip()) for line in f if line.strip()]\n","\n","image_folder = 'Data/Data/Images'\n","image_filenames = sorted(os.listdir(image_folder))  # Ensure alphabetical order\n","img_paths = [os.path.join(image_folder, fname) for fname in image_filenames if fname.lower().endswith(('.png'))]"]},{"cell_type":"markdown","id":"3036a4f8","metadata":{"id":"3036a4f8"},"source":["## Split training dataset"]},{"cell_type":"code","execution_count":null,"id":"8924edab","metadata":{"id":"8924edab"},"outputs":[],"source":["val_size = 0.2\n","test_size = 0.125\n","is_shuffle = True\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    img_paths, steer_values,\n","    test_size=val_size,\n","    shuffle=is_shuffle\n",")\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_train, y_train,\n","    test_size=test_size,\n","    shuffle=is_shuffle\n",")"]},{"cell_type":"markdown","id":"19b9a467","metadata":{"id":"19b9a467"},"source":["## Define dataset"]},{"cell_type":"code","execution_count":null,"id":"585478b7","metadata":{"id":"585478b7"},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((220, 220)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","])\n","\n","class SteeringDataset(Dataset):\n","    def __init__(self, img_paths, steer_values, transform=None):\n","      # make sure check the length between folders: images and number of steer values\n","        self.img_paths = img_paths\n","        self.steer_values = steer_values\n","        self.transform = transform if transform is not None else transforms.ToTensor()\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        img_path = self.img_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","\n","        # Apply transformation\n","        image = self.transform(image)\n","\n","        # Convert steering value to tensor\n","        steer_value = torch.tensor(self.steer_values[idx], dtype=torch.float32)\n","\n","        return image, steer_value\n"]},{"cell_type":"markdown","id":"f0a2ee6b","metadata":{"id":"f0a2ee6b"},"source":["## Define dataloader"]},{"cell_type":"code","execution_count":null,"id":"d4efe5be","metadata":{"id":"d4efe5be"},"outputs":[],"source":["# Create dataset instances\n","train_dataset = SteeringDataset(X_train, y_train, transform=transform)\n","val_dataset   = SteeringDataset(X_val, y_val, transform=transform)\n","test_dataset  = SteeringDataset(X_test, y_test, transform=transform)\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","id":"5c6c199c","metadata":{"id":"5c6c199c"},"source":["## Define model"]},{"cell_type":"code","execution_count":null,"id":"99eda32b","metadata":{"id":"99eda32b"},"outputs":[],"source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_channels, out_channels,\n","            kernel_size=3, stride=stride, padding=1, bias=False\n","        )\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.conv2 = nn.Conv2d(\n","            out_channels, out_channels,\n","            kernel_size=3, stride=1, padding=1, bias=False\n","        )\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        # Downsample if input and output dimensions do not match\n","        self.downsample = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.downsample = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","    def forward(self, x):\n","        identity = x\n","        out = self.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.downsample(identity)\n","        out = self.relu(out)\n","        return out\n","\n","\n","class ResNetSteering(nn.Module):\n","    def __init__(self, block, layers):\n","        super(ResNetSteering, self).__init__()\n","        self.in_channels = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self._make_layer(block, 64,  layers[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.flatten = nn.Flatten()\n","        self.dropout = nn.Dropout(0.3)\n","        self.fc = nn.Linear(512, 1)  # Single output neuron for regression\n","\n","    def _make_layer(self, block, out_channels, num_blocks, stride):\n","        layers = []\n","        layers.append(block(self.in_channels, out_channels, stride))\n","        self.in_channels = out_channels\n","        for _ in range(1, num_blocks):\n","            layers.append(block(out_channels, out_channels))\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.relu(self.bn1(self.conv1(x)))\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = self.flatten(x)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x.squeeze(1)  # Optional: return shape (batch,) instead of (batch, 1)"]},{"cell_type":"markdown","id":"4824d690","metadata":{"id":"4824d690"},"source":["## Call out model"]},{"cell_type":"code","execution_count":null,"id":"239d73e8","metadata":{"id":"239d73e8"},"outputs":[],"source":["device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","\n","model = ResNetSteering(ResidualBlock, [2, 2, 2, 2]).to(device)\n","\n"]},{"cell_type":"markdown","id":"0f658a0e","metadata":{"id":"0f658a0e"},"source":["## Define evaluation function"]},{"cell_type":"code","execution_count":null,"id":"3195dc58","metadata":{"id":"3195dc58"},"outputs":[],"source":["def evaluate(model, dataLoader, criterion, device):\n","    model.eval()\n","    losses = []\n","    total_abs_error = 0.0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in dataLoader:\n","            inputs, labels = inputs.to(device), labels.to(device).float()\n","\n","            outputs = model(inputs).squeeze()\n","            labels = labels.squeeze()\n","\n","            loss = criterion(outputs, labels)\n","            losses.append(loss.item())\n","\n","            total_abs_error += torch.sum(torch.abs(outputs - labels)).item()\n","            total_samples += labels.size(0)\n","\n","    avg_loss = sum(losses) / len(losses)\n","    mae = total_abs_error / total_samples\n","    return avg_loss, mae"]},{"cell_type":"code","execution_count":null,"id":"0c98aad3","metadata":{"id":"0c98aad3"},"outputs":[],"source":["def fit(\n","    model,\n","    train_loader,\n","    val_loader,\n","    criterion,\n","    optimizer,\n","    scheduler,\n","    device,\n","    epochs,\n","    model_save_path=\"best_model.pth\"\n","):\n","    train_losses = []\n","    val_losses = []\n","    val_maes = []\n","\n","    best_val_mae = float(\"inf\")\n","\n","    for epoch in range(epochs):\n","        batch_train_losses = []\n","        model.train()\n","\n","        for idx, (inputs, labels) in enumerate(train_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device).float()\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs).squeeze()\n","            labels = labels.squeeze()\n","\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            batch_train_losses.append(loss.item())\n","\n","        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n","        train_losses.append(train_loss)\n","\n","        val_loss, val_mae = evaluate(model, val_loader, criterion, device)\n","        val_losses.append(val_loss)\n","        val_maes.append(val_mae)\n","\n","        scheduler.step()\n","\n","        # === Save model if MAE improves ===\n","        if val_mae < best_val_mae:\n","            best_val_mae = val_mae\n","            torch.save(model.state_dict(), model_save_path)\n","            print(f\"Model saved at epoch {epoch+1} with best MAE: {val_mae:.4f}\")\n","\n","        print(f'EPOCH {epoch+1}: '\n","              f'Train_loss: {train_loss:.4f}\\t '\n","              f'Val_loss: {val_loss:.4f}\\t '\n","              f'Val_MAE: {val_mae:.4f}')\n","\n","    return train_losses, val_losses, val_maes"]},{"cell_type":"markdown","id":"cbb39d90","metadata":{"id":"cbb39d90"},"source":["## Train"]},{"cell_type":"code","execution_count":null,"id":"133bf001","metadata":{"id":"133bf001","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d40b8cd0-0b9e-4158-e367-0c11a8e92503"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved at epoch 1 with best MAE: 0.0248\n","EPOCH 1: Train_loss: 0.0047\t Val_loss: 0.0019\t Val_MAE: 0.0248\n","Model saved at epoch 2 with best MAE: 0.0223\n","EPOCH 2: Train_loss: 0.0027\t Val_loss: 0.0016\t Val_MAE: 0.0223\n","Model saved at epoch 3 with best MAE: 0.0200\n","EPOCH 3: Train_loss: 0.0017\t Val_loss: 0.0013\t Val_MAE: 0.0200\n","Model saved at epoch 4 with best MAE: 0.0146\n","EPOCH 4: Train_loss: 0.0015\t Val_loss: 0.0008\t Val_MAE: 0.0146\n","Model saved at epoch 5 with best MAE: 0.0114\n","EPOCH 5: Train_loss: 0.0010\t Val_loss: 0.0006\t Val_MAE: 0.0114\n","EPOCH 6: Train_loss: 0.0010\t Val_loss: 0.0006\t Val_MAE: 0.0117\n","Model saved at epoch 7 with best MAE: 0.0101\n","EPOCH 7: Train_loss: 0.0006\t Val_loss: 0.0005\t Val_MAE: 0.0101\n","Model saved at epoch 8 with best MAE: 0.0088\n","EPOCH 8: Train_loss: 0.0005\t Val_loss: 0.0004\t Val_MAE: 0.0088\n","EPOCH 9: Train_loss: 0.0004\t Val_loss: 0.0004\t Val_MAE: 0.0092\n","Model saved at epoch 10 with best MAE: 0.0085\n","EPOCH 10: Train_loss: 0.0003\t Val_loss: 0.0004\t Val_MAE: 0.0085\n","Model saved at epoch 11 with best MAE: 0.0079\n","EPOCH 11: Train_loss: 0.0003\t Val_loss: 0.0003\t Val_MAE: 0.0079\n","Model saved at epoch 12 with best MAE: 0.0071\n","EPOCH 12: Train_loss: 0.0002\t Val_loss: 0.0003\t Val_MAE: 0.0071\n","EPOCH 13: Train_loss: 0.0002\t Val_loss: 0.0003\t Val_MAE: 0.0072\n","Model saved at epoch 14 with best MAE: 0.0067\n","EPOCH 14: Train_loss: 0.0002\t Val_loss: 0.0002\t Val_MAE: 0.0067\n","EPOCH 15: Train_loss: 0.0002\t Val_loss: 0.0002\t Val_MAE: 0.0069\n","Model saved at epoch 16 with best MAE: 0.0065\n","EPOCH 16: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0065\n","Model saved at epoch 17 with best MAE: 0.0056\n","EPOCH 17: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0056\n","Model saved at epoch 18 with best MAE: 0.0055\n","EPOCH 18: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0055\n","EPOCH 19: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0056\n","EPOCH 20: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0055\n","EPOCH 21: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0055\n","Model saved at epoch 22 with best MAE: 0.0052\n","EPOCH 22: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0052\n","Model saved at epoch 23 with best MAE: 0.0050\n","EPOCH 23: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0050\n","EPOCH 24: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0052\n","Model saved at epoch 25 with best MAE: 0.0050\n","EPOCH 25: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0050\n","Model saved at epoch 26 with best MAE: 0.0047\n","EPOCH 26: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0047\n","Model saved at epoch 27 with best MAE: 0.0045\n","EPOCH 27: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0045\n","Model saved at epoch 28 with best MAE: 0.0045\n","EPOCH 28: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0045\n","Model saved at epoch 29 with best MAE: 0.0045\n","EPOCH 29: Train_loss: 0.0001\t Val_loss: 0.0002\t Val_MAE: 0.0045\n","Model saved at epoch 30 with best MAE: 0.0044\n","EPOCH 30: Train_loss: 0.0000\t Val_loss: 0.0001\t Val_MAE: 0.0044\n","EPOCH 31: Train_loss: 0.0000\t Val_loss: 0.0002\t Val_MAE: 0.0044\n","Model saved at epoch 32 with best MAE: 0.0043\n","EPOCH 32: Train_loss: 0.0000\t Val_loss: 0.0001\t Val_MAE: 0.0043\n"]}],"source":["lr = 1e-3\n","epochs = 50\n","\n","def lr_lambda(epoch, warmup_epochs=5, total_epochs=30,\n","              init_scale=0.1, min_scale=0.3):\n","    scale_range = 1.0 - min_scale\n","    if epoch < warmup_epochs:\n","        warmup_factor = epoch / warmup_epochs\n","        return init_scale + (1.0 - init_scale) * warmup_factor\n","    decay_factor = (total_epochs - epoch) / (total_epochs - warmup_epochs)\n","    return min_scale + scale_range * max(0.0, decay_factor)\n","\n","# Loss for regression\n","criterion = nn.MSELoss()\n","\n","# Optimizer and scheduler\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","# Train\n","train_losses, val_losses, val_maes = fit(\n","    model,\n","    train_loader,\n","    val_loader,\n","    criterion,\n","    optimizer,\n","    scheduler,\n","    device,\n","    epochs\n",")\n","\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}